# %% [code] {"execution":{"iopub.status.busy":"2025-06-04T14:16:03.013927Z","iopub.execute_input":"2025-06-04T14:16:03.014427Z","iopub.status.idle":"2025-06-04T14:16:03.365863Z","shell.execute_reply.started":"2025-06-04T14:16:03.014378Z","shell.execute_reply":"2025-06-04T14:16:03.364894Z"}}
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import warnings
warnings.filterwarnings('ignore')

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

# %% [markdown]
# # Data loading and preprocessing

# %% [code] {"execution":{"iopub.status.busy":"2025-06-04T14:17:25.546626Z","iopub.execute_input":"2025-06-04T14:17:25.546964Z","iopub.status.idle":"2025-06-04T14:17:25.593766Z","shell.execute_reply.started":"2025-06-04T14:17:25.546943Z","shell.execute_reply":"2025-06-04T14:17:25.593124Z"}}
df = pd.read_csv('/kaggle/input/zepto-inventory-dataset/zepto_v2.csv',encoding='latin1')
df

# %% [code] {"execution":{"iopub.status.busy":"2025-06-04T14:17:47.773952Z","iopub.execute_input":"2025-06-04T14:17:47.774214Z","iopub.status.idle":"2025-06-04T14:17:47.799656Z","shell.execute_reply.started":"2025-06-04T14:17:47.774194Z","shell.execute_reply":"2025-06-04T14:17:47.798870Z"}}
df.info()

# %% [code] {"execution":{"iopub.status.busy":"2025-06-04T14:18:04.247923Z","iopub.execute_input":"2025-06-04T14:18:04.248726Z","iopub.status.idle":"2025-06-04T14:18:04.256381Z","shell.execute_reply.started":"2025-06-04T14:18:04.248655Z","shell.execute_reply":"2025-06-04T14:18:04.255257Z"}}
df.isna().sum()

# %% [markdown]
# * Excel file

# %% [code] {"execution":{"iopub.status.busy":"2025-06-04T14:18:42.713541Z","iopub.execute_input":"2025-06-04T14:18:42.713895Z","iopub.status.idle":"2025-06-04T14:18:43.835714Z","shell.execute_reply.started":"2025-06-04T14:18:42.713872Z","shell.execute_reply":"2025-06-04T14:18:43.834443Z"}}
df_ex = pd.read_excel('/kaggle/input/zepto-inventory-dataset/zepto_v1.xlsx')
df_ex

# %% [markdown]
# # Vizualization Section

# %% [code] {"execution":{"iopub.status.busy":"2025-06-04T14:20:44.148351Z","iopub.execute_input":"2025-06-04T14:20:44.148834Z","iopub.status.idle":"2025-06-04T14:20:44.994611Z","shell.execute_reply.started":"2025-06-04T14:20:44.148810Z","shell.execute_reply":"2025-06-04T14:20:44.993396Z"}}
import matplotlib.pyplot as plt
import seaborn as sns

# %% [code] {"execution":{"iopub.status.busy":"2025-06-04T14:21:00.570519Z","iopub.execute_input":"2025-06-04T14:21:00.570901Z","iopub.status.idle":"2025-06-04T14:21:00.576000Z","shell.execute_reply.started":"2025-06-04T14:21:00.570882Z","shell.execute_reply":"2025-06-04T14:21:00.574997Z"}}
sns.set_style('whitegrid')

# %% [code] {"execution":{"iopub.status.busy":"2025-06-04T14:23:36.194258Z","iopub.execute_input":"2025-06-04T14:23:36.194529Z","iopub.status.idle":"2025-06-04T14:23:36.199637Z","shell.execute_reply.started":"2025-06-04T14:23:36.194509Z","shell.execute_reply":"2025-06-04T14:23:36.198185Z"}}
cols = ['Category', 'name', 'mrp', 'discountPercent', 'availableQuantity',
       'discountedSellingPrice', 'weightInGms', 'outOfStock', 'quantity']

# %% [markdown]
# # Histogrma plot

# %% [code] {"execution":{"iopub.status.busy":"2025-06-04T14:23:46.254825Z","iopub.execute_input":"2025-06-04T14:23:46.255076Z","iopub.status.idle":"2025-06-04T14:24:07.081195Z","shell.execute_reply.started":"2025-06-04T14:23:46.255056Z","shell.execute_reply":"2025-06-04T14:24:07.080297Z"}}
for features in cols:
    if features in df.columns:
        plt.figure(figsize=(20,5))
        sns.histplot(df[features].dropna(),kde=True)
        plt.title(f"Histogram plot of {features}")
        plt.xlabel(features)
        plt.ylabel("Count")
        plt.tight_layout()
        plt.xticks(rotation=60)
        plt.show()
        

# %% [code] {"execution":{"iopub.status.busy":"2025-06-04T14:24:38.288352Z","iopub.execute_input":"2025-06-04T14:24:38.288686Z","iopub.status.idle":"2025-06-04T14:24:38.298650Z","shell.execute_reply.started":"2025-06-04T14:24:38.288646Z","shell.execute_reply":"2025-06-04T14:24:38.297956Z"}}
df.info()

# %% [markdown]
# # Feature Engineering

# %% [code] {"execution":{"iopub.status.busy":"2025-06-04T14:25:27.541065Z","iopub.execute_input":"2025-06-04T14:25:27.541371Z","iopub.status.idle":"2025-06-04T14:25:27.546004Z","shell.execute_reply.started":"2025-06-04T14:25:27.541353Z","shell.execute_reply":"2025-06-04T14:25:27.544888Z"}}
lab_cols = ['Category','name','outOfStock']

# %% [code] {"execution":{"iopub.status.busy":"2025-06-04T14:24:22.653206Z","iopub.execute_input":"2025-06-04T14:24:22.654310Z","iopub.status.idle":"2025-06-04T14:24:22.787876Z","shell.execute_reply.started":"2025-06-04T14:24:22.654276Z","shell.execute_reply":"2025-06-04T14:24:22.786816Z"}}
from sklearn.preprocessing import LabelEncoder

# %% [code] {"execution":{"iopub.status.busy":"2025-06-04T14:24:33.008955Z","iopub.execute_input":"2025-06-04T14:24:33.009247Z","iopub.status.idle":"2025-06-04T14:24:33.014149Z","shell.execute_reply.started":"2025-06-04T14:24:33.009227Z","shell.execute_reply":"2025-06-04T14:24:33.012976Z"}}
le = LabelEncoder()

# %% [code] {"execution":{"iopub.status.busy":"2025-06-04T14:26:08.095962Z","iopub.execute_input":"2025-06-04T14:26:08.096265Z","iopub.status.idle":"2025-06-04T14:26:08.106417Z","shell.execute_reply.started":"2025-06-04T14:26:08.096243Z","shell.execute_reply":"2025-06-04T14:26:08.105164Z"}}
for column in lab_cols:
    df[column] = le.fit_transform(df[column])

# %% [code] {"execution":{"iopub.status.busy":"2025-06-04T14:26:10.172938Z","iopub.execute_input":"2025-06-04T14:26:10.174140Z","iopub.status.idle":"2025-06-04T14:26:10.184332Z","shell.execute_reply.started":"2025-06-04T14:26:10.174108Z","shell.execute_reply":"2025-06-04T14:26:10.183418Z"}}
df

# %% [markdown]
# # ANN Section

# %% [code] {"execution":{"iopub.status.busy":"2025-06-04T14:40:03.204070Z","iopub.execute_input":"2025-06-04T14:40:03.204378Z","iopub.status.idle":"2025-06-04T14:40:03.210779Z","shell.execute_reply.started":"2025-06-04T14:40:03.204357Z","shell.execute_reply":"2025-06-04T14:40:03.208871Z"}}
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score,r2_score,mean_absolute_error,mean_squared_error,classification_report

# %% [code] {"execution":{"iopub.status.busy":"2025-06-04T14:27:57.233821Z","iopub.execute_input":"2025-06-04T14:27:57.234104Z","iopub.status.idle":"2025-06-04T14:28:19.371479Z","shell.execute_reply.started":"2025-06-04T14:27:57.234084Z","shell.execute_reply":"2025-06-04T14:28:19.370212Z"}}
import tensorflow as tf

# %% [code] {"execution":{"iopub.status.busy":"2025-06-04T14:29:10.376852Z","iopub.execute_input":"2025-06-04T14:29:10.377378Z","iopub.status.idle":"2025-06-04T14:29:21.032035Z","shell.execute_reply.started":"2025-06-04T14:29:10.377359Z","shell.execute_reply":"2025-06-04T14:29:21.031336Z"}}
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Input
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.optimizers import Adam
import shap

# %% [code] {"execution":{"iopub.status.busy":"2025-06-04T14:32:30.003194Z","iopub.execute_input":"2025-06-04T14:32:30.003471Z","iopub.status.idle":"2025-06-04T14:32:30.008867Z","shell.execute_reply.started":"2025-06-04T14:32:30.003453Z","shell.execute_reply":"2025-06-04T14:32:30.007919Z"}}
X = df.drop(df.columns[-2],axis=1)
y = df.iloc[:,-2]


# %% [code] {"execution":{"iopub.status.busy":"2025-06-04T14:39:18.319224Z","iopub.execute_input":"2025-06-04T14:39:18.319529Z","iopub.status.idle":"2025-06-04T14:39:18.327714Z","shell.execute_reply.started":"2025-06-04T14:39:18.319511Z","shell.execute_reply":"2025-06-04T14:39:18.326554Z"}}
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)

# %% [code] {"execution":{"iopub.status.busy":"2025-06-04T14:33:39.336449Z","iopub.execute_input":"2025-06-04T14:33:39.336790Z","iopub.status.idle":"2025-06-04T14:33:39.368982Z","shell.execute_reply.started":"2025-06-04T14:33:39.336772Z","shell.execute_reply":"2025-06-04T14:33:39.368263Z"}}
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# %% [code] {"execution":{"iopub.status.busy":"2025-06-04T14:35:58.967068Z","iopub.execute_input":"2025-06-04T14:35:58.967375Z","iopub.status.idle":"2025-06-04T14:35:59.060715Z","shell.execute_reply.started":"2025-06-04T14:35:58.967342Z","shell.execute_reply":"2025-06-04T14:35:59.059577Z"}}
model = Sequential([
    Input(shape=(X_train_scaled.shape[1],)),
    Dense(128,activation='relu'),
    Dense(64,activation='relu'),
    Dense(1)
])

# %% [code] {"execution":{"iopub.status.busy":"2025-06-04T14:40:08.194205Z","iopub.execute_input":"2025-06-04T14:40:08.194639Z","iopub.status.idle":"2025-06-04T14:40:12.754952Z","shell.execute_reply.started":"2025-06-04T14:40:08.194609Z","shell.execute_reply":"2025-06-04T14:40:12.753634Z"}}
model.compile(optimizer=Adam(),loss='mean_squared_error',metrics=['mse'])
model.fit(X_train_scaled,y_train,epochs=10,batch_size=32,validation_split=0.1)


y_pred = model.predict(X_test_scaled)
y_pred_labels = (y_pred > 0.5).astype(int).flatten()

print(classification_report(y_test,y_pred_labels))

# %% [code] {"execution":{"iopub.status.busy":"2025-06-04T14:45:27.430102Z","iopub.execute_input":"2025-06-04T14:45:27.430431Z","iopub.status.idle":"2025-06-04T14:45:27.813861Z","shell.execute_reply.started":"2025-06-04T14:45:27.430405Z","shell.execute_reply":"2025-06-04T14:45:27.812826Z"}}
# Predict
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

# Convert probabilities to class labels
y_train_pred_labels = (y_train_pred > 0.5).astype(int)
y_test_pred_labels = (y_test_pred > 0.5).astype(int)

# Evaluate
from sklearn.metrics import classification_report

print("Train Report:")
print(classification_report(y_train, y_train_pred_labels))

print("Test Report:")
print(classification_report(y_test, y_test_pred_labels))


# %% [markdown]
# Conclusion:
# 
# >Current ANN is severely underperforming and overfitting to the minority class.
# 
# >You should address class imbalance and retrain with class_weight, SMOTE, or both.
# 
# >After fixing, re-run the classification report to see improvements in both recall and precision across both classes.

# %% [code]
